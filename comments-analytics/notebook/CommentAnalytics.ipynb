{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Sentiment Analysis, Text Summarization & Visualisation"}, {"metadata": {}, "cell_type": "markdown", "source": "## Setup\nTo prepare your environment, you need to install some packages.\n\n### Install the necessary packages\n\nYou need the latest versions of these packages:<br>"}, {"metadata": {}, "cell_type": "code", "source": "!pip install gensim", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install watson-developer-cloud==1.5", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install pyldavis", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install wordcloud", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from gensim.summarization.summarizer import summarize\nfrom gensim.summarization import keywords\nimport watson_developer_cloud\n# import ibm_boto3\n# from botocore.client import Config\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport urllib\nfrom bs4 import BeautifulSoup\nimport requests\nimport nltk\nnltk.download('all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Summarization & keywords extraction"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1a. Helper functions to extract summary and keywords"}, {"metadata": {}, "cell_type": "code", "source": "'''Get the summary of the text'''\n\ndef get_summary(text, pct):\n    summary = summarize(text,ratio=pct,split=True)\n    return summary\n\ndef complete_summary(summary):\n    summary = \" \".join(summary)\n    print(type(summary))\n    return summary\n\n'''Get the keywords of the text'''\n\ndef get_keywords(text):\n    res = keywords(text, ratio=0.1, words=None, split=False, scores=False, pos_filter=('NN', 'JJ'), lemmatize=False, deacc=False)\n    res = res.split('\\n')\n    return res\n\n'''Tokenize the sentence into words & remove punctuation'''\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n        \ndef split_sentences(text):\n    \"\"\" Split text into sentences.\n    \"\"\"\n    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n    sentences = sentence_delimiters.split(text)\n    return sentences\n\ndef split_into_tokens(text):\n    \"\"\" Split text into tokens.\n    \"\"\"\n    tokens = nltk.word_tokenize(text)\n    return tokens\n    \ndef POS_tagging(text):\n    \"\"\" Generate Part of speech tagging of the text.\n    \"\"\"\n    POSofText = nltk.tag.pos_tag(text)\n    return POSofText\n\ndef extract_title_text(url):\n    page = urllib.request.urlopen(url).read().decode('utf8')\n    soup = BeautifulSoup(page,'lxml')\n    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n    return soup.title.text, text", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1b. Summarization & keywords extraction"}, {"metadata": {}, "cell_type": "code", "source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_6b4ef4db85984d6cb9f05ef0da73c427 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='Mfkrien7_WF2rNDAEHoJEHHrKvXHZANIcPRqsHeuZPFW',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_6b4ef4db85984d6cb9f05ef0da73c427.get_object(Bucket='submissionanalysis-donotdelete-pr-jdnlxbx7mat2e1',Key='ProblemSolutionDBData.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face an error during data loading.\n# Please read the documentation of 'pandas.read_json()' and 'pandas.io.json.json_normalize' to learn more about the possibilities to adjust the data loading.\n# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\n# and http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n\n#print(body)\n#df_data_1 = pd.read_json(body, orient='values')\ndf_data_1 = pd.read_json(body)\n#df_data_1.head()\nrawtext = ' '\nproblem = df_data_1[\"PROBLEM\"][\"uploads\"][1][\"text\"]\nsolution = df_data_1[\"SOLUTION\"][\"uploads\"][1][\"text\"]\ncomments = df_data_1['SOLUTION'][\"comments\"]\nfor c in comments:\n    rawtext = rawtext + c['comment'] + ' '\n#print(rawtext)\n\nprint ('-------------------------------------------------------------------------------------------------------------------')\nprint('Printing Summary of review comments')\nprint('--------------------------')\n\nsummary = get_summary(rawtext, 0.13)\nprint(summary)\n\nprint ('-------------------------------------------------------------------------------------------------------------------')\nprint ('-------------------------------------------------------------------------------------------------------------------')\nprint('Printing Keywords')\nprint('--------------------------')\nhashtag_list =[]\nfor i in get_keywords(rawtext):\n    hashtag = '#'+i\n    hashtag_list.append(hashtag)\n    \nhashtag_string = str(\" \".join(hashtag_list))\nprint(hashtag_string)\nprint ('-------------------------------------------------------------------------------------------------------------------')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2. Topic Modelling"}, {"metadata": {}, "cell_type": "markdown", "source": "# 2a. Start the preprocessing for Topic Modelling\n\nTopic Modelling is an approach for finding topics in large amounts of text. Topic modeling is great for document clustering, information retrieval from unstructured text, and feature selection.\n \nTopic Modeling with Latent Dirichlet Allocation technique.\n\nWhy Latent Dirichlet Allocation? This technique can create model which can be generalized easily on any new text corpus and help us in identifying the important topics from the corpus. \n\nSome of the advantages are :\n\nTraining documents may come in sequentially, no random access required.\n\nRuns in constant memory w.r.t. the number of documents: size of the training corpus does not affect memory footprint, can process corpora larger than RAM.\n\nIs distributed & makes use of a cluster of machines, if available, to speed up model estimation."}, {"metadata": {}, "cell_type": "code", "source": "# article_text = summary\nstop_words = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()\nfilteredtext = rawtext\nfilteredtext = filteredtext.replace('the', '')\nfilteredtext = filteredtext.replace('It', '')\nfilteredtext = filteredtext.replace('may', '')\nfilteredtext = filteredtext.replace('maybe', '')\nfilteredtext = filteredtext.replace('wish', '')\n\nword_tokens = word_tokenize(str(filteredtext)) \nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nnormalized = \" \".join(lemma.lemmatize(word) for word in filtered_sentence)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 3. Visualization"}, {"metadata": {}, "cell_type": "code", "source": "from wordcloud import WordCloud\n#wordcloud = WordCloud(background_color='white',max_font_size=60).generate(normalized)\n\nwordcloud = WordCloud(background_color=\"white\", max_words=70, contour_width=10, contour_color='firebrick').generate(normalized)\n\nplt.figure(figsize=(16,12))\n\n'''plot wordcloud in matplotlib'''\n\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4. Create Topic Model"}, {"metadata": {}, "cell_type": "code", "source": "tokenized_sents = list(sent_to_words(filtered_sentence))\nprint(tokenized_sents)\n\n# Creating the term dictionary of our corpus, where every unique term is assigned an index. \ndictionary = corpora.Dictionary(tokenized_sents)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_sents]\nprint(doc_term_matrix)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# Running and Training LDA model on the document term matrix.\nldamodel = Lda(doc_term_matrix, num_topics=1, id2word = dictionary, passes=30)\n\n# Print the model output\ntopics = ldamodel.print_topics(num_words=10)\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(topics)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in topics:\n    print(i[1].split('\"')[0])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print ('---------------------------------------')\ntweet_with_summary_hashtags = complete_summary(summary) + \" \"+hashtag_string\nprint(len(tweet_with_summary_hashtags))\nprint(tweet_with_summary_hashtags)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Coherence score is 'higher the better' metric and given the score of 0.86 we can be assured that we have selected the right number of topics for this corpus."}, {"metadata": {}, "cell_type": "code", "source": "'''Compute Perplexity'''\n\n# a measure of how good the model is. Lower the better.\nprint('\\nPerplexity: ', ldamodel.log_perplexity(doc_term_matrix))\n\n'''Compute Coherence Score'''\n\ncoherence_model_lda = CoherenceModel(model=ldamodel, texts=tokenized_sents, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### We have seen how to summarize & visualize review comments of a submitted solution and to create hash tags to get quick information about the data. This methodology can be applied to lot of usecases to extract insights from unstructured data."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}